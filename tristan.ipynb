{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Classical Time Series Models\n",
    "Great for small datasets or explainability.\n",
    "\n",
    "1. ARIMA / SARIMA\n",
    "Univariate\n",
    "\n",
    "Good for individual customer time series\n",
    "\n",
    "Doesn't handle multivariate inputs\n",
    "\n",
    "2. Exponential Smoothing (Holt-Winters)\n",
    "Good for capturing trends/seasonality\n",
    "\n",
    "Works best on individual customer series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Machine Learning Models\n",
    "Good if you extract features (lags, trends, rolling stats, etc.)\n",
    "\n",
    "4. Random Forest / Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "Treat it like a regression problem\n",
    "\n",
    "Need feature engineering (lagged values, time of day/week, etc.)\n",
    "\n",
    "Good for tabular-style learning\n",
    "\n",
    "5. Linear Regression (with engineered features)\n",
    "Works better than you'd think if features are well-designed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Deep Learning Models\n",
    "Best when lots of data and long-term dependencies matter.\n",
    "\n",
    "6. LSTM / GRU (Recurrent Neural Networks)\n",
    "Sequence models, good for capturing temporal dependencies\n",
    "\n",
    "Can be univariate or multivariate\n",
    "\n",
    "PyTorch and TensorFlow support these\n",
    "\n",
    "7. Temporal Convolutional Networks (TCN)\n",
    "Alternative to RNNs\n",
    "\n",
    "Often train faster and are easier to tune\n",
    "\n",
    "8. Transformer-based Models\n",
    "Powerful for long-range dependencies\n",
    "\n",
    "Libraries: PyTorch Forecasting, HuggingFace Timeseries Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"data/datasets2025/historical_metering_data_IT.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Create a new DataFrame where no columns have any NaN values\n",
    "df = df.dropna(axis=1, how='any')\n",
    "# print(df[target_col].isna().sum())\n",
    "# for col in df_no_nans.columns:\n",
    "#     if df_no_nans[col].isna().sum() > 0:\n",
    "#         print(f\"Column {col} has {df_no_nans[col].isna().sum()} missing values\")\n",
    "\n",
    "# Choose ONE customer for now (e.g., customerES_1)\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerIT_6\"\n",
    "series = df[[\"DATETIME\", target_col]].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.01125\n",
      "Epoch 2, Loss: 0.00302\n",
      "Epoch 3, Loss: 0.00266\n",
      "Epoch 4, Loss: 0.00252\n",
      "Epoch 5, Loss: 0.00242\n",
      "Epoch 6, Loss: 0.00239\n",
      "Epoch 7, Loss: 0.00232\n",
      "Epoch 8, Loss: 0.00231\n",
      "Epoch 9, Loss: 0.00228\n",
      "Epoch 10, Loss: 0.00225\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.81446767].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     68\u001b[39m     pred = model(last_seq.unsqueeze(\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     pred_rescaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted next value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_rescaled[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/tzerweck/Neural-ODEs-for-NQS/Neural-ODEs-for-NQS/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:573\u001b[39m, in \u001b[36mMinMaxScaler.inverse_transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    569\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    571\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m X -= \u001b[38;5;28mself\u001b[39m.min_\n\u001b[32m    582\u001b[39m X /= \u001b[38;5;28mself\u001b[39m.scale_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/tzerweck/Neural-ODEs-for-NQS/Neural-ODEs-for-NQS/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1093\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1086\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1087\u001b[39m             msg = (\n\u001b[32m   1088\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1091\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1092\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUSV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1097\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1098\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1099\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array, got 1D array instead:\narray=[0.81446767].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Normalize target values\n",
    "series[\"value\"] = scaler.fit_transform(series[[target_col]])\n",
    "# ------------------------------\n",
    "# 2. Create Dataset\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, seq_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = series[\"value\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "seq_len = 24  # e.g., using 24 hours of history to predict the next hour\n",
    "dataset = TimeSeriesDataset(series, seq_len=seq_len)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model\n",
    "# ------------------------------\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2):\n",
    "        super(LSTMForecast, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out.squeeze()\n",
    "\n",
    "model = LSTMForecast()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "for epoch in range(10):  # increase epochs for better performance\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Evaluation\n",
    "# ------------------------------\n",
    "\n",
    "# Use the last available sequence to predict the next value\n",
    "last_seq = torch.tensor(series[\"value\"].values[-seq_len:], dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(last_seq.unsqueeze(0))\n",
    "    pred_rescaled = scaler.inverse_transform(pred.unsqueeze(0).numpy())\n",
    "    print(f\"Predicted next value: {pred_rescaled[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTISTEP FORECASTING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"your_data.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Choose ONE customer for now (e.g., customerES_1)\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerES_1\"\n",
    "series = df[[\"DATETIME\", target_col]].dropna()\n",
    "\n",
    "# Normalize target values\n",
    "scaler = MinMaxScaler()\n",
    "series[\"value\"] = scaler.fit_transform(series[[target_col]])\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset for multi-step forecasting\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, seq_len=24, forecast_horizon=12):\n",
    "        self.seq_len = seq_len\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.data = series[\"value\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.forecast_horizon\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.forecast_horizon]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "seq_len = 24  # Using 24 hours of history to predict the next 12 hours\n",
    "forecast_horizon = 12  # Number of steps to forecast ahead\n",
    "dataset = TimeSeriesDataset(series, seq_len=seq_len, forecast_horizon=forecast_horizon)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model for Multi-Step Forecasting\n",
    "# ------------------------------\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, forecast_horizon=12):\n",
    "        super(LSTMForecast, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, forecast_horizon)  # output forecast_horizon steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step and predict forecast_horizon steps\n",
    "        return out\n",
    "\n",
    "model = LSTMForecast(forecast_horizon=forecast_horizon)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "for epoch in range(10):  # increase epochs for better performance\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Evaluation\n",
    "# ------------------------------    \n",
    "\n",
    "# Use the last available sequence to predict the next forecast_horizon steps\n",
    "last_seq = torch.tensor(series[\"value\"].values[-seq_len:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Forecast using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    forecast = model(last_seq)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast.squeeze(0).numpy().reshape(-1, 1))\n",
    "\n",
    "    # Print or plot the forecast\n",
    "    print(f\"Predicted next {forecast_horizon} values: {forecast_rescaled.flatten()}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the last observed values and the predicted future values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(len(series)), scaler.inverse_transform(series[\"value\"].values.reshape(-1, 1)), label=\"Historical Data\")\n",
    "plt.plot(np.arange(len(series), len(series) + forecast_horizon), forecast_rescaled.flatten(), label=\"Forecast\", color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Multi-Step Forecast')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # out: [batch, seq_len, hidden]\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class DummySequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=20, input_size=1):\n",
    "        self.x = torch.randn(num_samples, seq_len, input_size)\n",
    "        self.y = torch.randn(num_samples, 1)  # regression target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = DummySequenceDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, device, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
