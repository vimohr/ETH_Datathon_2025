{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Classical Time Series Models\n",
    "Great for small datasets or explainability.\n",
    "\n",
    "1. ARIMA / SARIMA\n",
    "Univariate\n",
    "\n",
    "Good for individual customer time series\n",
    "\n",
    "Doesn't handle multivariate inputs\n",
    "\n",
    "2. Exponential Smoothing (Holt-Winters)\n",
    "Good for capturing trends/seasonality\n",
    "\n",
    "Works best on individual customer series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Machine Learning Models\n",
    "Good if you extract features (lags, trends, rolling stats, etc.)\n",
    "\n",
    "4. Random Forest / Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "Treat it like a regression problem\n",
    "\n",
    "Need feature engineering (lagged values, time of day/week, etc.)\n",
    "\n",
    "Good for tabular-style learning\n",
    "\n",
    "5. Linear Regression (with engineered features)\n",
    "Works better than you'd think if features are well-designed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Deep Learning Models\n",
    "Best when lots of data and long-term dependencies matter.\n",
    "\n",
    "6. LSTM / GRU (Recurrent Neural Networks)\n",
    "Sequence models, good for capturing temporal dependencies\n",
    "\n",
    "Can be univariate or multivariate\n",
    "\n",
    "PyTorch and TensorFlow support these\n",
    "\n",
    "7. Temporal Convolutional Networks (TCN)\n",
    "Alternative to RNNs\n",
    "\n",
    "Often train faster and are easier to tune\n",
    "\n",
    "8. Transformer-based Models\n",
    "Powerful for long-range dependencies\n",
    "\n",
    "Libraries: PyTorch Forecasting, HuggingFace Timeseries Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tristan_helpers import *\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"data/datasets2025/historical_metering_data_IT.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Clean the dataset: drop rows with NaN values in the target column\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerIT_6\"\n",
    "df = df.dropna(axis =1, how= 'any')\n",
    "\n",
    "# Normalize the target column\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df[\"value\"] = scaler.fit_transform(df[[target_col]])\n",
    "\n",
    "# Split into train and test sets\n",
    "test_size = 31 * 24  # 30 days * 24 hours = 720 hours\n",
    "train_data = df[:-test_size]\n",
    "test_data = df[-test_size:]\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset Class\n",
    "# ------------------------------\n",
    "\n",
    "# Create DataLoader for training\n",
    "seq_len = 24*7\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len=seq_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMForecast().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "# Training loop with progress bar\n",
    "progress_bar = tqdm(range(500), desc=\"Training Epochs\")\n",
    "epoch_losses = []\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Plot the Loss\n",
    "# ------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_losses, label=\"Training Loss\", color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Evaluation\n",
    "# ------------------------------\n",
    "\n",
    "threshold = 0.05  # 5% accuracy threshold\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "accurate_predictions = 0\n",
    "total_predictions = len(test_data) - seq_len\n",
    "skipped_indices = 0\n",
    "not_skipped_indices = 0\n",
    "\n",
    "for idx in range(seq_len, len(test_data)):  # Start from seq_len because we need previous data for prediction\n",
    "    last_seq = torch.tensor(test_data[\"value\"].values[idx-seq_len:idx], dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(last_seq.unsqueeze(0))  # Make the prediction\n",
    "        \n",
    "        # Move tensor to CPU and reshape for inverse scaling\n",
    "        pred_rescaled = scaler.inverse_transform(pred.cpu().unsqueeze(0).reshape(-1, 1))\n",
    "        \n",
    "    actual_value = test_data[target_col].iloc[idx]\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if actual_value != 0:\n",
    "        error = abs(pred_rescaled[0][0] - actual_value)\n",
    "        if error / actual_value <= threshold:\n",
    "            accurate_predictions += 1\n",
    "            not_skipped_indices += 1\n",
    "    else:\n",
    "        skipped_indices += 1\n",
    "        continue\n",
    "accuracy = (accurate_predictions / total_predictions) * 100\n",
    "print(f\"Model accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Percentage of skipped indices due to zero actual value: {skipped_indices/(skipped_indices + not_skipped_indices)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 247/500 [02:44<02:51,  1.48it/s, loss=0.00036]"
     ]
    }
   ],
   "source": [
    "## chat updates\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tristan_helpers import *  # Assuming the necessary helper functions are here\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"data/datasets2025/historical_metering_data_IT.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Clean the dataset: drop rows with NaN values in the target column\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerIT_6\"\n",
    "df = df.dropna(subset=[target_col])  # Drop rows with NaN in the target column\n",
    "\n",
    "# Normalize the target column\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df[\"value\"] = scaler.fit_transform(df[[target_col]])\n",
    "\n",
    "# Split into train and test sets\n",
    "test_size = 31 * 24  # 30 days * 24 hours = 720 hours\n",
    "train_data = df[:-test_size]\n",
    "test_data = df[-test_size:]\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset Class\n",
    "# ------------------------------\n",
    "\n",
    "# Create DataLoader for training\n",
    "seq_len = 24\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len=seq_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model\n",
    "# ------------------------------\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMForecast().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "# Training loop with progress bar\n",
    "progress_bar = tqdm(range(500), desc=\"Training Epochs\")\n",
    "epoch_losses = []\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Plot the Loss\n",
    "# ------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_losses, label=\"Training Loss\", color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Evaluation\n",
    "# ------------------------------\n",
    "\n",
    "threshold = 0.05  # 5% accuracy threshold\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "accurate_predictions = 0\n",
    "total_predictions = len(test_data) - seq_len\n",
    "skipped_indices = 0\n",
    "not_skipped_indices = 0\n",
    "\n",
    "for idx in range(seq_len, len(test_data)):  # Start from seq_len because we need previous data for prediction\n",
    "    last_seq = torch.tensor(test_data[\"value\"].values[idx-seq_len:idx], dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(last_seq.unsqueeze(0))  # Make the prediction\n",
    "        \n",
    "        # Move tensor to CPU and reshape for inverse scaling\n",
    "        pred_rescaled = scaler.inverse_transform(pred.cpu().unsqueeze(0).reshape(-1, 1))\n",
    "        \n",
    "    actual_value = test_data[target_col].iloc[idx]\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if actual_value != 0:\n",
    "        error = abs(pred_rescaled[0][0] - actual_value)\n",
    "        if error / actual_value <= threshold:\n",
    "            accurate_predictions += 1\n",
    "            not_skipped_indices += 1\n",
    "    else:\n",
    "        skipped_indices += 1\n",
    "        continue\n",
    "\n",
    "accuracy = (accurate_predictions / total_predictions) * 100\n",
    "print(f\"Model accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Percentage of skipped indices due to zero actual value: {skipped_indices/(skipped_indices + not_skipped_indices)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"data/datasets2025/historical_metering_data_IT.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Create a new DataFrame where no columns have any NaN values\n",
    "df = df.dropna(axis=1, how='any')\n",
    "\n",
    "# Choose ONE customer for now (e.g., customerES_1)\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerIT_6\"\n",
    "series = df[[\"DATETIME\", target_col]].dropna()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Normalize target values\n",
    "series[\"value\"] = scaler.fit_transform(series[[target_col]])\n",
    "\n",
    "# Split Data (Train/Test)\n",
    "# Define test size as the last month's data\n",
    "test_size = 30 * 24  # 30 days * 24 hours = 720 hours\n",
    "\n",
    "# Split dataset\n",
    "train_data = series[:-test_size]  # All data except the last month's data\n",
    "test_data = series[-test_size:]  # Last month's data\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset for Multi-Step Forecasting\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset:\n",
    "    def __init__(self, series, seq_len=24, forecast_horizon=3):\n",
    "        self.seq_len = seq_len\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.data = series[\"value\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.forecast_horizon\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]  # these are the input features\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.forecast_horizon]  # target sequence to predict\n",
    "        return x, y\n",
    "\n",
    "seq_len = 24  # e.g., using 24 hours of history to predict the next hours\n",
    "forecast_horizon = 3  # Predicting the next 3 hours\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len=seq_len, forecast_horizon=forecast_horizon)\n",
    "test_dataset = TimeSeriesDataset(test_data, seq_len=seq_len, forecast_horizon=forecast_horizon)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model in Flax (For Multi-Step)\n",
    "# ------------------------------\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    hidden_dim: int\n",
    "    forecast_horizon: int\n",
    "    \n",
    "    def setup(self):\n",
    "        # Layers of the LSTM network\n",
    "        self.lstm_cell = nn.LSTMCell(name=\"lstm_cell\")\n",
    "        self.dense = nn.Dense(self.forecast_horizon)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        state = self.lstm_cell.initialize_carry(jax.random.PRNGKey(0), batch_size, x.shape[1])\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            state, _ = self.lstm_cell(state, x[:, t])\n",
    "            outputs.append(state[0])\n",
    "        \n",
    "        output = jnp.stack(outputs, axis=1)\n",
    "        return self.dense(output[:, -1])  # Only take the last output from LSTM\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Loss Function and Gradients (for Multi-Step)\n",
    "# ------------------------------\n",
    "\n",
    "def loss_fn(params, model, x, y):\n",
    "    pred = model.apply({\"params\": params}, x)\n",
    "    return jnp.mean((pred - y) ** 2)  # MSE loss\n",
    "\n",
    "grad_loss_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Training Loop with JAX and Flax (for Multi-Step)\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize parameters\n",
    "model = LSTMForecast(hidden_dim=64, forecast_horizon=forecast_horizon)\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((1, seq_len)))  # Initialize model\n",
    "\n",
    "# Optimizer (using optax)\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    total_loss = 0\n",
    "    for idx in range(len(train_dataset)):\n",
    "        x, y = train_dataset[idx]\n",
    "\n",
    "        # Convert data to JAX arrays and reshape as needed\n",
    "        x = jnp.array(x).reshape(1, seq_len)\n",
    "        y = jnp.array(y).reshape(1, forecast_horizon)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss, grads = grad_loss_fn(params, model, x, y)\n",
    "\n",
    "        # Update the parameters\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    # Update the progress bar\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Evaluation for Multi-Step Forecasting\n",
    "# ------------------------------\n",
    "\n",
    "# Define accuracy threshold (e.g., 5%)\n",
    "threshold = 0.05  # 5% accuracy threshold\n",
    "\n",
    "accurate_predictions = 0\n",
    "total_predictions = len(test_dataset)\n",
    "\n",
    "for idx in range(len(test_dataset)):\n",
    "    x, y = test_dataset[idx]\n",
    "\n",
    "    # Convert data to JAX arrays and reshape as needed\n",
    "    x = jnp.array(x).reshape(1, seq_len)\n",
    "    y = jnp.array(y).reshape(1, forecast_horizon)\n",
    "\n",
    "    # Make multi-step prediction\n",
    "    pred = model.apply({\"params\": params}, x)\n",
    "    \n",
    "    # Rescale prediction and true value\n",
    "    pred_rescaled = scaler.inverse_transform(pred.reshape(-1, 1))  # Rescale the prediction\n",
    "    actual_rescaled = scaler.inverse_transform(y.reshape(-1, 1))  # Rescale the actual value\n",
    "    \n",
    "    # Calculate error for each forecasted step\n",
    "    error = jnp.abs(pred_rescaled - actual_rescaled)\n",
    "\n",
    "    # Check if each of the forecasted steps is within the accuracy threshold\n",
    "    for i in range(forecast_horizon):\n",
    "        if error[i] / actual_rescaled[i] <= threshold:\n",
    "            accurate_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (accurate_predictions / total_predictions) * 100\n",
    "print(f\"Model accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
