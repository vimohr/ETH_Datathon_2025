{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Classical Time Series Models\n",
    "Great for small datasets or explainability.\n",
    "\n",
    "1. ARIMA / SARIMA\n",
    "Univariate\n",
    "\n",
    "Good for individual customer time series\n",
    "\n",
    "Doesn't handle multivariate inputs\n",
    "\n",
    "2. Exponential Smoothing (Holt-Winters)\n",
    "Good for capturing trends/seasonality\n",
    "\n",
    "Works best on individual customer series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Machine Learning Models\n",
    "Good if you extract features (lags, trends, rolling stats, etc.)\n",
    "\n",
    "4. Random Forest / Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "Treat it like a regression problem\n",
    "\n",
    "Need feature engineering (lagged values, time of day/week, etc.)\n",
    "\n",
    "Good for tabular-style learning\n",
    "\n",
    "5. Linear Regression (with engineered features)\n",
    "Works better than you'd think if features are well-designed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Deep Learning Models\n",
    "Best when lots of data and long-term dependencies matter.\n",
    "\n",
    "6. LSTM / GRU (Recurrent Neural Networks)\n",
    "Sequence models, good for capturing temporal dependencies\n",
    "\n",
    "Can be univariate or multivariate\n",
    "\n",
    "PyTorch and TensorFlow support these\n",
    "\n",
    "7. Temporal Convolutional Networks (TCN)\n",
    "Alternative to RNNs\n",
    "\n",
    "Often train faster and are easier to tune\n",
    "\n",
    "8. Transformer-based Models\n",
    "Powerful for long-range dependencies\n",
    "\n",
    "Libraries: PyTorch Forecasting, HuggingFace Timeseries Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.62s/it, loss=0.00226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted first hour of the last month: 0.29612\n",
      "Actual first hour of the last month: 0.18406\n",
      "Predicted next value: 0.17338642830580475\n",
      "Predicted first hour of the last month: 0.17339\n",
      "Actual first hour of the last month: 0.18406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"data/datasets2025/historical_metering_data_IT.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Create a new DataFrame where no columns have any NaN values\n",
    "df = df.dropna(axis=1, how='any')\n",
    "# print(df[target_col].isna().sum())\n",
    "# for col in df_no_nans.columns:\n",
    "#     if df_no_nans[col].isna().sum() > 0:\n",
    "#         print(f\"Column {col} has {df_no_nans[col].isna().sum()} missing values\")\n",
    "\n",
    "# Choose ONE customer for now (e.g., customerES_1)\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerIT_6\"\n",
    "series = df[[\"DATETIME\", target_col]].dropna()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Normalize target values\n",
    "series[\"value\"] = scaler.fit_transform(series[[target_col]])\n",
    "\n",
    "# Split Data (Train/Test)\n",
    "# Define test size as the last month's data\n",
    "test_size = 30 * 24  # 30 days * 24 hours = 720 hours\n",
    "\n",
    "# Split dataset\n",
    "train_data = series[:-test_size]  # All data except the last month's data\n",
    "test_data = series[-test_size:]  # Last month's data\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, seq_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = series[\"value\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len] # these are the input features\n",
    "        y = self.data[idx + self.seq_len] # this is the target value you want to predict\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "seq_len = 24  # e.g., using 24 hours of history to predict the next hour\n",
    "dataset = TimeSeriesDataset(series, seq_len=seq_len)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model\n",
    "# ------------------------------\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2):\n",
    "        super(LSTMForecast, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out.squeeze()\n",
    "\n",
    "model = LSTMForecast()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "# Create a tqdm instance\n",
    "progress_bar = tqdm(range(10), desc=\"Training Epochs\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Update the tqdm bar with the loss value\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.5f}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Evaluation\n",
    "# ------------------------------\n",
    "\n",
    "# Use the last available sequence to predict the next value\n",
    "last_seq = torch.tensor(train_data[\"value\"].values[-seq_len:], dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(last_seq.unsqueeze(0))  # Make the prediction\n",
    "    pred_rescaled = scaler.inverse_transform(pred.unsqueeze(0).reshape(-1, 1))  # Reshape to 2D\n",
    "    print(f\"Predicted next value: {pred_rescaled[0][0]}\")\n",
    "\n",
    "# Get the first actual value from the test set (first hour of the last month)\n",
    "first_actual_value = test_data[target_col].iloc[0]\n",
    "\n",
    "# Print the predicted vs actual value\n",
    "print(f\"Predicted first hour of the last month: {pred_rescaled[0][0]:.5f}\")\n",
    "print(f\"Actual first hour of the last month: {first_actual_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTISTEP FORECASTING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# ------------------------------\n",
    "\n",
    "df = pd.read_csv(\"your_data.csv\", parse_dates=[\"DATETIME\"])\n",
    "df = df.sort_values(\"DATETIME\")\n",
    "\n",
    "# Choose ONE customer for now (e.g., customerES_1)\n",
    "target_col = \"VALUEMWHMETERINGDATA_customerES_1\"\n",
    "series = df[[\"DATETIME\", target_col]].dropna()\n",
    "\n",
    "# Normalize target values\n",
    "scaler = MinMaxScaler()\n",
    "series[\"value\"] = scaler.fit_transform(series[[target_col]])\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Dataset for multi-step forecasting\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, seq_len=24, forecast_horizon=12):\n",
    "        self.seq_len = seq_len\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.data = series[\"value\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.forecast_horizon\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.forecast_horizon]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "seq_len = 24  # Using 24 hours of history to predict the next 12 hours\n",
    "forecast_horizon = 12  # Number of steps to forecast ahead\n",
    "dataset = TimeSeriesDataset(series, seq_len=seq_len, forecast_horizon=forecast_horizon)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LSTM Model for Multi-Step Forecasting\n",
    "# ------------------------------\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, forecast_horizon=12):\n",
    "        super(LSTMForecast, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, forecast_horizon)  # output forecast_horizon steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step and predict forecast_horizon steps\n",
    "        return out\n",
    "\n",
    "model = LSTMForecast(forecast_horizon=forecast_horizon)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "for epoch in range(10):  # increase epochs for better performance\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.5f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Evaluation\n",
    "# ------------------------------    \n",
    "\n",
    "# Use the last available sequence to predict the next forecast_horizon steps\n",
    "last_seq = torch.tensor(series[\"value\"].values[-seq_len:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Forecast using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    forecast = model(last_seq)\n",
    "    forecast_rescaled = scaler.inverse_transform(forecast.squeeze(0).numpy().reshape(-1, 1))\n",
    "\n",
    "    # Print or plot the forecast\n",
    "    print(f\"Predicted next {forecast_horizon} values: {forecast_rescaled.flatten()}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the last observed values and the predicted future values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(len(series)), scaler.inverse_transform(series[\"value\"].values.reshape(-1, 1)), label=\"Historical Data\")\n",
    "plt.plot(np.arange(len(series), len(series) + forecast_horizon), forecast_rescaled.flatten(), label=\"Forecast\", color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Multi-Step Forecast')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # out: [batch, seq_len, hidden]\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class DummySequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=20, input_size=1):\n",
    "        self.x = torch.randn(num_samples, seq_len, input_size)\n",
    "        self.y = torch.randn(num_samples, 1)  # regression target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = DummySequenceDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, device, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
